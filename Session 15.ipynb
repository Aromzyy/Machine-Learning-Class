{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNS, filters, and deep learning\n",
    "This class covers convolutional neural nets. If you have any troubles running your code  locally, you can find the full notebook here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 of 7\n",
    "Question 1: CNN Intro (2)\n",
    "\n",
    "\n",
    "Engineer two prompts for ChatGPT or a similar LLM: \n",
    "\n",
    "\n",
    "\n",
    "One that produces a good high-level explanation of how a CNN works, and \n",
    "One that produces a good step-by-step explanation of the mechanism of convolution and how you might use convolution to converts images. \n",
    "\n",
    "\n",
    "Include your entire dialog with the LLM below, including both of these prompts and your suggestions for improvements.\n",
    "\n",
    "\n",
    "\n",
    "Once you've primed it with your two prompts. Ask the LLM this follow-up question:\n",
    "\n",
    "\n",
    "\n",
    "In 150 words or less, describe the \"tied weights\" property of CNNs. Specifically, how are CNNs just neural networks where the weights are \"shared\" between features? Why is the chain rule of differentiation is especially important calculating parameters in deep networks such a convolutional neural network?\n",
    "\n",
    "\n",
    "Q: Give me a high level explanation on how a cnn works (100 words)\n",
    "\n",
    "A: A Convolutional Neural Network (CNN) is a deep learning algorithm that can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image, and differentiate one from the other. The pre-processing required in a CNN is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, CNNs have the ability to learn these filters/characteristics. The architecture of a CNN is analogous to that of the connectivity pattern of Neurons in the Human Brain and was inspired by the organization of the Visual Cortex.\n",
    "\n",
    "\n",
    "\n",
    "Q: Now, give me a detailed, step by step explanation of how a CNN works\n",
    "\n",
    "A: Certainly! Convolutional Neural Networks (CNNs) are a class of deep neural networks, most commonly applied to analyzing visual imagery. They have revolutionized the field of computer vision, achieving remarkable performance in tasks such as image classification, object detection, and more. Here's a detailed, step-by-step explanation of how CNNs work:\n",
    "\n",
    "\n",
    "\n",
    "1. **Input Layer:**\n",
    "\n",
    "  - The input layer holds the raw pixel values of the image. For instance, an image of size 200x200 pixels will have 40,000 neurons in the input layer, with each neuron representing the intensity of a pixel.\n",
    "\n",
    "\n",
    "\n",
    "2. **Convolution Layer:**\n",
    "\n",
    "  - The primary purpose of the convolution layer is to detect features such as edges, colors, or textures in the input image.\n",
    "\n",
    "  - Filters (also known as kernels) slide (or \"convolve\") across the input image and perform element-wise multiplication with the part of the image they are covering.\n",
    "\n",
    "  - The sum of the multiplication is taken, and the result forms a single pixel in the output feature map (or activation map).\n",
    "\n",
    "  - Multiple filters are used to extract different features, resulting in multiple feature maps.\n",
    "\n",
    "\n",
    "\n",
    "3. **ReLU (Rectified Linear Unit) Layer:**\n",
    "\n",
    "  - This layer applies a non-linear activation function to the output of the convolution layer. The most common function used is ReLU, which replaces all negative pixel values in the feature map with zero.\n",
    "\n",
    "  - The purpose of ReLU is to introduce non-linearity into the system, allowing the network to solve more complex problems.\n",
    "\n",
    "\n",
    "\n",
    "4. **Pooling (Subsampling or Downsampling) Layer:**\n",
    "\n",
    "  - This layer reduces the spatial dimensions (width and height) of the input volume for the next convolutional layer. It reduces the number of parameters, which helps in reducing computation and also helps in making the detection of features invariant to scale and orientation changes.\n",
    "\n",
    "  - Max pooling is a common technique used in pooling where the maximum element from the rectified feature map is taken.\n",
    "\n",
    "\n",
    "\n",
    "5. **Fully Connected (FC) Layer:**\n",
    "\n",
    "  - After several convolutional and max pooling layers, the high-level reasoning in the neural network is done via fully connected layers.\n",
    "\n",
    "  - Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. Their activation can hence be computed with a matrix multiplication followed by a bias offset.\n",
    "\n",
    "\n",
    "\n",
    "6. **Flatten Layer:**\n",
    "\n",
    "  - The Flatten Layer converts the final feature maps into a single 1D vector. This flattening step is needed so that you can make use of fully connected layers after some convolution/maxpool layers.\n",
    "\n",
    "\n",
    "\n",
    "7. **Output Layer:**\n",
    "\n",
    "  - The last fully connected layer outputs the final probabilities for each class in a classification task (like softmax function), or the final features in a feature extraction task.\n",
    "\n",
    "\n",
    "\n",
    "8. **Backpropagation:**\n",
    "\n",
    "  - To learn the weights of the filters, CNNs use backpropagation, a form of supervised learning. After the forward pass, the network will compute the loss (i.e., the difference between the predicted output and the actual output).\n",
    "\n",
    "  - The network then uses this loss to update the filter values in a way that minimizes the loss, using an optimization technique like gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "By stacking several convolutional and pooling layers, CNNs are able to learn hierarchical feature representations of the input data. Lower layers might learn to detect simple features like edges and curves, while deeper layers can recognize complex features like faces or objects. This hierarchical feature extraction makes CNNs very effective for tasks like image recognition.\n",
    "\n",
    "\n",
    "\n",
    "Q: In 150 words or less, describe the \"tied weights\" property of CNNs. Specifically, how are CNNs just neural networks where the weights are \"shared\" between features? Why is the chain rule of differentiation is especially important calculating parameters in deep networks such a convolutional neural network?\n",
    "\n",
    "\n",
    "\n",
    "A: \"Tied weights\" in CNNs refer to the concept of weight sharing, a key property that differentiates them from fully connected neural networks. In a CNN, the same filter (a set of weights) is applied across different parts of the input, effectively sharing the weights. This means that instead of learning separate weights for every pixel in the image, the network learns filters that detect features (like edges or textures) irrespective of their position in the input. This significantly reduces the number of parameters, making CNNs more efficient and less prone to overfitting.\n",
    "\n",
    "\n",
    "\n",
    "The chain rule of differentiation is crucial in training deep networks like CNNs because it allows for the computation of gradients with respect to the weights during backpropagation. In a CNN, due to the depth and weight sharing, gradients of the loss function with respect to the weights are calculated by applying the chain rule iteratively through each layer. This enables the network to update its weights (including shared weights) in a way that minimizes the loss, effectively learning the feature representations in a hierarchical manner.\n",
    "\n",
    "\n",
    "\n",
    "Critique: \n",
    "\n",
    "While the answer succinctly encapsulates the essence of \"tied weights\" in CNNs and their implications, such as parameter efficiency and reduced overfitting, some aspects of it could be improved. For instance, a more concrete example or analogy could be provided to illustrate the concept of weight sharing. \n",
    "\n",
    "Additionally, the role of the chain rule could be clarified further. While it mentions how important it is for computing gradients, it would be beneficial to emphasize that the chain rule enables the propagation of error information from the output back to each layer, which is essential for adjusting the shared weights appropriately during training.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Question: Convolutions (3)\n",
    "In the readings, you looked at how a convolution layer is simply a \"stack\" of filters applied over an image to create a family of \"new\" images. In the pre-class, we'll be looking at the \"one filter\" level to build an intuition of how convolution works. Here, we use the \"Racoon face\" image that comes as a default image in scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from the docs \n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.correlate2d.html\n",
    "from scipy import signal\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "face = misc.face(gray=True) - misc.face(gray=True).mean()\n",
    "plt.imshow(face, cmap=\"Greys\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All a convolution is is a \"template matching\" process; we give it a \"mini-image\", and it searches for instances of this mini-image in a larger image. Here, let's extract the right eye of the raccoon to use as a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = face[300:365, 670:750]\n",
    "template -= template.mean()\n",
    "\n",
    "plt.imshow(template, cmap=\"Greys\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we \"convolve\" this template over the image. Note that strictly speaking, in machine learning we use \"cross-correlation\", instead of convolution. They're both similar, but technically speaking a convolution searches for \"upside down\" instances of a template, whereas cross-correlationn searches for the template as provided. This split is primarily due to convolutions arising from the signal processing community first, and then being used in machine learning.\n",
    "\n",
    "\n",
    "\n",
    "Tl;dr in machine learning when we say \"convolution\" we really mean \"cross-correlation\". And as we see, there is a sharp activation (darker is stronger) near the right eye of the racoon, right where the original template was taken from!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = signal.correlate2d(face, template, boundary='symm', mode='same')\n",
    "plt.imshow(corr, cmap=\"Greys\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Question 2a: \n",
    "What is this image? Why are there strong activations in locations other than the right eye (where the template is from), such as the left eye and the nose?\n",
    "\n",
    "The image is the output of template matching, which is used to locate and recognize parts of an image that match a particular pattern or template. \n",
    "\n",
    "The heatmap shows the matching scores, with brighter areas indicating higher similarity to the template. \n",
    "\n",
    "The strong responses at locations other than the right eye, from where the template is derived, suggest that the left eye and the nose have similar features to the right eye—like edges, shapes, or textures—that the simple template matching algorithm is responding to. This can result in false positives, highlighting a limitation of basic template matching techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also try other \"filters\": the following is known as the Sobel operator, a classic edge detector in computer vision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n",
    "\n",
    "plt.imshow(template, cmap=\"Greys\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applied over the original image, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = signal.correlate2d(face, template, boundary='symm', mode='same')\n",
    "plt.imshow(corr, cmap=\"Greys\")\n",
    "print(\"Operator 1\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2b: Is this a horizontal or a vertical edge detector? Why have you chosen the answer that you have?\n",
    "\n",
    "\n",
    "\n",
    "Hint: Look the Sobel operator itself above (the 3x3 grid image). In which way is it oriented? What \"pattern\" would activate it?\n",
    "\n",
    "\n",
    "\n",
    "Question 2c: We've only looked at grayscale images here (that is, only one channel), but most real-world images are in color! Based on the Shafkat (2018) reading, how is the match with a \"filter/kernel\" computed when the image has more than one \"channel\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2b: Is this a horizontal or a vertical edge detector? Why have you chosen the answer that you have?\n",
    "\n",
    "This is a horizontal edge detector. This is because of how it interacts with the image.\n",
    "\n",
    "\n",
    "\n",
    "Rows (Top to Bottom):\n",
    "\n",
    "\n",
    "\n",
    "The top row ([1, 0, -1]) has positive values at the left and a negative value at the right. This row will respond to dark-to-light transitions from left to right across the top of the area it is convolved with.\n",
    "\n",
    "The middle row ([2, 0, -2]) is weighted more heavily (values are doubled) compared to the top and bottom rows, which increases the sensitivity of the filter to horizontal edges. The positive values are again on the left and the negative on the right, just like the top row.\n",
    "\n",
    "The bottom row ([1, 0, -1]) is similar to the top row but flipped in sign, responding to light-to-dark transitions from left to right across the bottom of the convolved area.\n",
    "\n",
    "\n",
    "\n",
    "Columns (Left to Right):\n",
    "\n",
    "\n",
    "\n",
    "The left column ([1, 2, 1]) has all positive values and weights the filter to respond to light areas on the left when scanning the image.\n",
    "\n",
    "The middle column ([0, 0, 0]) has zero values, meaning it does not respond to vertical changes in intensity—this is crucial for detecting horizontal edges since horizontal edges will not cause a change in intensity along the vertical axis within a small area.\n",
    "\n",
    "The right column ([-1, -2, -1]) has all negative values, mirroring the left column, and weights the filter to respond to dark areas on the right.\n",
    "\n",
    "\n",
    "\n",
    "Because of this, when the filter is applied (convolved) over the image, areas with strong horizontal changes in intensity (from light to dark or dark to light) will produce strong responses. These areas correspond to horizontal edges within the image, such as the boundaries between different objects or features.\n",
    "\n",
    "\n",
    "\n",
    "Question 2c: We've only looked at grayscale images here (that is, only one channel), but most real-world images are in color! Based on the Shafkat (2018) reading, how is the match with a \"filter/kernel\" computed when the image has more than one \"channel\"?\n",
    "\n",
    "According to Shafkat (2018), when dealing with color images, which typically have three channels (Red, Green, and Blue), the matching with a filter or kernel is done by extending the filter to have the same depth as the number of channels.  It can be done as follows:\n",
    "\n",
    "\n",
    "\n",
    "Channel Extension - The filter is extended across the depth of the input image. For a color image with three channels, this means we will have a 3-dimensional filter. If the original grayscale filter is 3x3, the color filter would be 3x3x3.\n",
    "\n",
    "\n",
    "\n",
    "Filter Application - The extended filter is applied to the image by computing the dot product between the filter and the image pixels over all channels. This is done for every location on the image. \n",
    "\n",
    "\n",
    "\n",
    "Summation - After the dot product is computed across all channels, the results are summed up to produce a single value. This summation collapses the channel dimension, yielding a two-dimensional array as the output.\n",
    "\n",
    "\n",
    "\n",
    "Activation Map Creation - The single value from the summation is then placed in the corresponding location of the activation map (feature map).\n",
    "\n",
    "\n",
    "\n",
    "Repetition - This process is repeated for each location on the input image, sliding the filter across the image both horizontally and vertically.\n",
    "\n",
    "\n",
    "\n",
    "Multiple Filters - If multiple filters are applied, each one will produce a separate activation map. When all the filters have been applied, the activation maps are stacked along the depth dimension to produce the output volume.\n",
    "\n",
    "\n",
    "\n",
    "This enables the network to learn filters that can detect features specific to a particular channel combination (such as the color and texture) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension Question: Sobel operators + Gradients (4)\n",
    "We've only looked at one of the two Sobel operators here. Read through the wiki page on Sobel operators, and implement the other kernel, and apply it over the image:\n",
    "\n",
    "What does the racoon image look like with the other kernel applied?\n",
    "How is it different from the one in question 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first kernel detects changes in intensity in the horizontal direction. This is why there are strong responses at places where the gradient of the image intensity changes vertically, such as the vertical edges.\n",
    "\n",
    "\n",
    "\n",
    "The second image emphasizes vertical edges. This operator detects changes in intensity in the vertical direction, which is why you see a strong response at places where the gradient of the image intensity changes horizontally.\n",
    "\n",
    "\n",
    "\n",
    "The differences between the two images show the orientation of the edges they are designed to detect. The first operator responds more to vertical lines and edges (like the sides of the raccoon's face), while the second responds more to horizontal lines and edges (like the transitions around the eyes and the nose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "plt.imshow(template, cmap=\"Greys\")\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "corr = signal.correlate2d(face, template, boundary='symm', mode='same')\n",
    "plt.imshow(corr, cmap=\"Greys\")\n",
    "print(\"Operator 2\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the other filter applied, you can combine both filtered images with the following code to get the image gradient. What does it look like (visualize it with imshow) and how is it different from either filter in isolation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image shows the gradient magnitude of the edges in the raccoon image by combining horizontal and vertical edge detections. \n",
    "\n",
    "Unlike the images produced by the individual horizontal or vertical filters, this combined image highlights the edges in all directions. The result is a more complete representation of the edges and textures in the image, with increased detail and sharper contrast at the boundaries of shapes. \n",
    "\n",
    "This makes the outlines of the raccoon's features more defined against the background, compared to either filter used in isolation. However, it may also amplify noise, making the image appear rougher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in here\n",
    "horizontal_kernel = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n",
    "vertical_kernel = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "\n",
    "horizontal_edges = signal.correlate2d(face, horizontal_kernel, boundary='symm', mode='same')\n",
    "vertical_edges = signal.correlate2d(face, vertical_kernel, boundary='symm', mode='same')\n",
    "\n",
    "gradient = np.sqrt(horizontal_edges**2 + vertical_edges**2)\n",
    "\n",
    "# visualize here\n",
    "plt.imshow(gradient, cmap='gray')\n",
    "plt.title('Gradient Magnitude')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MLDevelopment\n",
    "Review the class notion and do some synthesis: write some useful summary notes for a previous topic from the first half of class and upload them to the class notion. Include your synthesis below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SESSION 9: METRICS AND CROSS-VALIDATION\n",
    "\n",
    "Some key points from that class include:\n",
    "\n",
    "\n",
    "\n",
    "Training, Validation, and Test Sets \n",
    "\n",
    "  - The training set is used for the model to learn and identify patterns.\n",
    "\n",
    "  - The validation set helps tune the model's hyperparameters and compare different models.\n",
    "\n",
    "  - The test set is used for the final evaluation, providing an estimate of the model's performance on unseen data.\n",
    "\n",
    "\n",
    "\n",
    "Performance Metrics\n",
    "\n",
    "  - Accuracy - measures the overall correctness of the model.\n",
    "\n",
    "  - Precision - gauges the correctness of positive predictions.\n",
    "\n",
    "  - Recall - (or Sensitivity) assesses the coverage of actual positive cases.\n",
    "\n",
    "  - F1 Score - combines precision and recall into a single metric, giving a balance of both.\n",
    "\n",
    "  - ROC Curve -plots True Positive Rate against False Positive Rate at various threshold settings.\n",
    "\n",
    "  - AUC - reflects the likelihood of the model distinguishing between classes.\n",
    "\n",
    "\n",
    "\n",
    "Cross-Validation\n",
    "\n",
    "  - K-Fold Cross-Validation- divides the data into k subsets, using k-1 for training and the remaining for validation, rotating until each subset has been used for validation.\n",
    "\n",
    "  - Leave-One-Out Cross-Validation (LOOCV)- takes k to the extreme, where k equals the number of observations.\n",
    "\n",
    " - Procedure for cross-validation Includes shuffling the data, splitting into groups, iterating through each group as a test set, and summarizing the model's skill.\n",
    "\n",
    " - Data preparation and hyperparameter tuning should occur within the cross-validation loop to prevent data leakage.\n",
    "\n",
    "\n",
    "\n",
    "Bias-Variance Tradeoff \n",
    "\n",
    "  - Bias - refers to errors due to overly simplistic assumptions in the learning algorithm.\n",
    "\n",
    "  - Variance - reflects errors due to too much complexity in the learning algorithm.\n",
    "\n",
    "\n",
    "\n",
    "Points to Consider\n",
    "\n",
    "  - Certain methods like the Validation Set Approach can be variable or overestimate the test error.\n",
    "\n",
    "  - AUC doesn't account for the distribution of classes and treats all errors equally.\n",
    "\n",
    "  - Cross-validation techniques can be computationally expensive, particularly LOOCV.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
